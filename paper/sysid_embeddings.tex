\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage[]{corl_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}       % blackboard math symbols
\usepackage{amssymb}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{algpseudocode,algorithm,algorithmicx}
\usepackage{comment}
%\usepackage{subfigure}
\usepackage{subcaption}

\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage{tikz}
\usepackage{pgffor}
\usetikzlibrary{arrows,positioning}

% My commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\TODO}[1]{\textcolor{red}{\textbf{TODO: #1}}}
%\newcommand{\TODO}[1]{}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cS}{\mathcal{S}}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\sysid}{dynamics}
\newcommand{\blind}{\emph{blind}}
\newcommand{\plain}{\emph{plain}}
\newcommand{\extra}{\emph{extra}}
\newcommand{\embed}{\emph{E2ID}}
\newcommand{\traj}{\emph{traj}}
\newcommand{\embedfn}{e}
\newcommand{\idfn}{id}
\newcommand{\latent}{\cL}

\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\KH}[1]{\textcolor{blue}{ #1}}


\title{Embed to Identify: Learning an Abstract System Identification Space for Adaptive Control\\ with Deep Reinforcement Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  James A.~Preiss \\
  Department of Computer Science\\
  University of Southern California\\
  Los Angeles, CA 90089\\
  \texttt{japreiss@usc.edu} \\
  %% examples of more authors
  \And
  Gaurav S. Sukhatme \\
  Department of Computer Science\\
  University of Southern California\\
  Los Angeles, CA 90089\\
  \texttt{gaurav@usc.edu} \\
  \AND
  Karol Hausman \\
  Google Brain \\
  1600 Amphitheatre Parkway\\
  Mountain View, CA 94043 \\
  \texttt{karolhausman@google.com} \\
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}

\maketitle

\begin{abstract}
Agents trained with reinforcement learning are often brittle and fail to generalize to test environments with different dynamics.
This is particularly visible in the problem of simulation-to-real transfer where the test environment (real robot) is different from the simulation environment it was trained in.
Domain randomization can make an agent more robust,
but is limited to scenarios where a single policy is sufficient for all possible test environments.
For larger changes in dynamics, some form of system identification is necessary,
but identifying every dynamics parameter accurately is a difficult intermediate step to optimal behavior.
We introduce a policy architecture and training scheme that learn an abstract embedding space
that distills system identification information into a form that is both more useful and easier to estimate.
In addition, we propose an observability-promoting reward that encourages the policy to balance the task goal with behavior that aids system identification.
Our experiments demonstrate improved task performance and show desirable properties of the learned embedding.
\end{abstract}

\section{Introduction}

Reinforcement learning (RL) is a highly general framework for learning policies that make decisions sequentially in an environment.
RL is a compelling approach to robotic control problems
because the same learning algorithm and policy structure can be applied to a wide range of robot environments
without requiring per-environment manual effort or insight.
Yet, in contrast to the generality of RL itself,
the policies trained with RL are usually highly specialized and fail to generalize to other similar environments,
even when the differences are small~\citep{zhang-study-on-overfitting}.

Such overfitting to the training environment is problematic as it does not allow to transfer trained policies, e.g. from simulation to reality. Training in a fast simulator and deploying on a real robot can save time,
but since simulators cannot faithfully reproduce every detail of physical phenomena such as fluid dynamics, contacts or friction forces, policies must be able to generalize from the training to the test scenario.
The problem of overfitting to the training environment is important beyond simulation-to-real transfer -- e.g., when training a policy in a lab and deploying it on mass-produced robots with manufacturing variations, different wear ant tear, etc. that will affect dynamics.

Failure to generalize has become the subject attention in the RL community.
One of the most successful attempts to improve generalization is domain randomization of dynamics parameters~\citep{antonova-pivoting-corr17, zhu-RL-IL-diverse}
or visual attributes~\citep{sadeghi-cad2rl-rss17,tobin-domainrand-arxiv17,james-domain-xfer} during training, such that the learned policy is invariant to those changes in the environment.
Although domain randomization has demonstrated a surprisingly good generalization ability,
it is inherently limited by its main assumption that a single policy performs sufficiently well in all domains.
% Related approaches include ensembles of policies~\citep{actor-mimic,teh-distral},
% adversarial perturbations of state or observations~\citep{pinto-robust-adversarial-RL,huang-adversarial-attacks},
% and learning robust feature spaces~\citep{higgins-DARLA,bousmalis-domainseparation-nips16}.
% Fine-tuning\TODO{fine-tuning citation} or progressive networks~\citep{rusu-progressive-nets} 
% use a policy trained in simulation as a starting point for further learning in the test environment.
% Model-agnostic meta-learning (MAML)~\citep{finn-maml-icml17}
% explicitly prepares for the fine-tuning by unrolling the policy gradient update in the training objective.
% \TODO{end this paragraph.}


In this paper, we instead propose an approach that can specialize its behavior online according to environment conditions. 
We also introduce an observability-seeking behavior that encourages the agent to quickly obtain the information needed to specialize. 
We focus on scenarios where the variation in dynamics parameters
is large enough to impede domain randomization,
but small enough that updating the parameters of the policy at test time is not strictly necessary.
Our proposed architecture and training regime produce a policy
that adapts online in real time to a test system with unknown dynamics parameters.
Our framework is inspired by control-theoretic architectures for online system identification
similar to~\citet{yu-up-osi-rss17},
but instead of attempting to identify the true parameters of the system,
we learn an abstract embedding space that is both
1) useful to help the policy behave optimally, and
2) easy to identify from a short state-action trajectory.

We demonstrate experimentally that our learned embeddings can perform dimensionality reduction on unobservable subspaces of the dynamics parameters,
and separate nearby parameter values that require disjoint behavior styles.

\section{Related Work}
Work on RL generalization generally falls into one of two categories: those that improve \emph{a priori} generalization of the policy without data from the test environment,
and those that update the parameters of the policy \emph{a posteriori} using data from the test environment.

The most straightforward \emph{a priori} approach is domain randomization,
in which the parameters of the dynamics and/or observations are randomized during training,
but the final policy does not explicitly perform system identification at test time.
%
Randomization of dynamics parameters in simulation has been shown to increase generalization ability~\citep{antonova-pivoting-corr17, zhu-RL-IL-diverse}.
Random textures and colors in the visual input has been used for successful sim-to-real transfer
for indoor quadrotor navigation~\citep{sadeghi-cad2rl-rss17}
and simple manipulation tasks~\citep{tobin-domainrand-arxiv17,james-domain-xfer}.
These methods focus on the visual part of the problem and use higher-level control inputs
to abstract away physical details of the target system.
Ensemble approaches optimize for mean performance over a set of models~\citep{mordatch-ensemble-icra15}
or distill a set of expert policies into a single policy that performs well in all environments~\citep{actor-mimic,teh-distral}.
%
Other \emph{a priori} approaches include adversarial setups,
where a learned ``antagonist'' agent attempts to decrease the primary ``protagonist'' agent's reward
by injecting physical disturbances into the system~\citep{pinto-robust-adversarial-RL}
or perturbing the observation input in vision-based agents~\citep{huang-adversarial-attacks}.
\citet{higgins-DARLA} build a factorized generative model of the visual scene
and train the agent on this space instead of pixel inputs.


\emph{A posteriori} approaches assume that there will be a chance to interact with the test environment,
potentially with poor task performance,
and subsequently update the parameters of the policy.
These updates may happen once at test time, or concurrently during training.
%
\citet{rusu-progressive-nets} deploy the \emph{progressive nets} framework
where a neural network policy is trained in simulation, and new cross-connected layers are added
at test time to adapt.
%
Model-agnostic meta-learning (MAML)~\citep{finn-maml-icml17} incorporates the policy gradient update step directly in its training objective,
optimizing the policy to adapt to the test environment in a single gradient descent step after gathering a small amount of data.
\citet{clavera-maml-model} apply a similar framework to model-based RL.
%
\citet{duan-rl2} train a recurrent policy that updates its own parameters after interacting with the test environment.
%
Visual differences between training and a specific test environment have been addressed
using generative adversarial networks (GANs)~\citep{bousmalis-domain-gan-cvpr17}
or by learning a domain-invariant feature space~\citep{bousmalis-domainseparation-nips16}.
%
%\citet{christiano-deep-inverse-dynamics-corr16} learn an inverse dynamics model and use the trained policy in the simulator to select desired states.
%\TODO{characterize better.}
%
\citet{rajeswaran-epopt-corr16} train a policy with domain randomization, but
adapt the parameter distribution and reward weights based on interactions with the target domain.

Domain randomization is appealing in its simplicity, and has performed well in the literature and in our own experiments.
However, it implicitly assumes that, given the observed state of the system,
there exists an action that will produce acceptable behavior over all possible values
of the unknown system identification parameters.
This assumption may not hold if the set of possible test environments is diverse.
%
In cases where this assumption is violated, the policy must be able to behave differently in different environments,
and there must be a mechanism to extract some information about the hidden parameters of the environment during interaction.
This is the scenario considered in this paper.
A similar approach to ours is~\citet{yu-up-osi-rss17},
where the authors trained a policy that takes the dynamics parameters as input
and an online system identification module to estimate them.
We observe that estimating the true dynamics parameters requires more work than needed --
one can instead learn to map the dynamics parameters into an embedding space
that is more useful for the policy, as well as being easier to estimate.
~\citet{peng-dynamics-randomization-corr17} make a similar observation and use a recurrent policy architecture.
A recurrent architecture is able to extract information about the environment in its internal state without imposing any particular structure on the information that is extracted.
By comparison, our framework makes it possible to modify the training reward such that the model is explicitly encouraged to behave in a manner that makes the parameters easily observable.





\section{Problem Statement}

We consider reinforcement learning in a Markov Decision Process (MDP)
%where $\cS \subseteq \R^\cS$ is the state space,
%$\cA \subseteq \R^\cA$ is the action space,
%$p : \cS \times \cA \times \cS \mapsto \R_{\geq 0}$ are the stochastic dynamics,
%$\rho : \cS \mapsto \R_{\geq 0}$ is the initial state distribution,
%and $r : \cS \times \cA \mapsto \R$ is the task reward function.
with state space $\cS$,
action space $\cA$,
stochastic dynamics $p : \cS \times \cA \times \cS \mapsto \R_{\geq 0}$,
%stochastic dynamics $p(s_{t + 1} | s_t, a_t)$,
initial state distribution $\rho : \cS \mapsto \R_{\geq 0}$,
task reward function $r : \cS \times \cA \mapsto \R$,
and finite time horizon $H \in \mathbb{N}$.
The state space $\cS$ is partitioned into
the task space $\cS_T$
and a probability distribution over the dynamics space $\cS_D$, such that $\cS = \cS_T \times \cS_D$.
The task space $\cS_T$ consists of the intrinsic state of the robot, such as the angles and angular velocities of revolute joints,
as well as task specification inputs such as a goal position.
These states can be measured or accurately estimated in the real system.
The \sysid{} space $\cS_D$ consists of parameters that may be unmeasureable or difficult to estimate, e.g. joint geometries, moments of inertia, sensor and actuator characteristics, and coefficients of drag or friction.
We additionally overload the notation $\cS_D$ to refer to the probability distribution over these parameters.
We seek to learn an optimal stochastic policy maximizing the expected reward over all \sysid{} parameters:
\begin{equation}\begin{split}
\pi : \cS \times \cA \mapsto \R_{\geq 0}, \quad
\pi^\star = \argmax_\pi \E_{s_d \sim \cS_D,\ \tau \sim p(\tau|\pi,s_D)} \sum_{t = 0}^H
r(s_t, a_t),
\label{objective}
\end{split}\end{equation}
where $\tau = (s_0, a_0, \dots, a_{H-1}, s_H)$ denotes a state-action trajectory
and $p(\tau | \pi, s_D)$ denotes the probability of a trajectory $\tau$ in the given MDP under the policy $\pi$
and \sysid{} parameters $s_D$.
While training in the simulator, we assume that we have access to the ground truth \sysid{} parameters $s_d$.
However, during test time, the true \sysid{} parameters are unknown, which makes the approach applicable to real world scenarios.
%Due to the complexity and nonlinearity of real-world dynamics,
%small errors in the measurements of these states can result in major differences between the behavior of the system in reality and in simulation.

\section{Embed to Identify (E2ID) Framework}
A natural approach to the problem~\eqref{objective}, explored by~\citet{yu-up-osi-rss17},
is to learn a function $\idfn$ to estimate $s_d \in \cS_D$ from a state-action trajectory $\tau_K = (s_1, a_1), (s_2, a_2), \dots, (s_K, a_K)$.
The policy can be trained in simulation with known $s_d$;
at test time, an estimate of $s_d$ is obtained from $\idfn$ in real time using a fixed-length history buffer of actions and observations.
However, this approach presents two challenges.
%
First, it requires estimating every system identification parameter,
even though some may be difficult to estimate, redundant, or unneeded for the task.
An example of this issue is shown in \secref{pointmass},
where a mass and an actuator power are reduced to a single dimensionless quantity.
Reductions such as these can make the system identification problem easier, but they may not be easy to derive by hand on complex systems.
%
Second, the behavior that maximizes the expected reward~\eqref{objective}
in simulation with known $s_d$ may not be the best behavior for making the system identification task easier.
It is preferable to learn a behavior that balances the primary objective~\eqref{objective}
with a secondary objective of making the system identification task as easy as possible so that the agent can quickly identify how it should specialize its behavior.

In this work, we present a framework that addresses both of these concerns.
We introduce a learned function $\embedfn : \cS_D \mapsto \latent$
that maps the system identification parameters to an abstract latent space $\latent \subset \R^{d_L}$.
We then learn a function $\idfn : (\cS \times \cA)^K \mapsto \latent$, for $1 < K \ll H$,
that attempts to recover the value of the latent space by observing a state-action trajectory.
The latent space dimensionality $d_L$ and the window length $K$ are user-chosen hyperparameters.
The functions $\embedfn$ and $\idfn$ are learned simultaneously with the policy $\pi$ in an end-to-end fashion.
The latent space creates an opportunity to distill the full complexity of the system identification space $\cS_D$ into a space that is potentially more useful for the policy to produce optimal behavior,
while simultaneously being easier to estimate.

While the latent space may help make the system identification task easier,
it has a failure mode that must be considered:
if $\embedfn$ and $\idfn$ both map to the same constant value for any input,
$\idfn$ will achieve perfect performance during training,
but the embedding will not influence the policy, and $\pi$ will be reduced to an average policy
equivalent to that learned under pure domain randomization.
To avoid this problem, we impose a structure where the parameters of the embedding function $e$
can only be learned via the RL objective, which includes a term rewarding accurate system identification of $id$:
\begin{equation}\begin{split}
%\pi^\star &= \argmax_\pi 
J(\pi) = 
\E_{s_d \sim \cS_D,\ \tau \sim p(\tau|\pi)}
\left[
\sum_{t = 0}^H r(s_t, a_t)
- \alpha_{id} \sum_{t = 0}^{H-K} \| \idfn(\tau_{t:t+K}) - \embedfn(s_d) \|_2^2
%+ \alpha_{id} \E_{\tau \sim \text{\TODO{complicated}}} \| id(\tau) - e(s_d) \|_2 \\
\right]
+ \alpha_{\cH}\cH(\pi)
%&+ \alpha_{KL} KL(e(s_d), \cN(0,1))
\label{objective-full}
\end{split}\end{equation}
where $\tau_{t:t+K}$ denotes the $K$-length subtrajectory of $\tau$ starting at time $t$.
In the modified objective~\eqref{objective-full},
we emphasize that the $\alpha_{id}$ term is part of the reinforcement learning reward
for the policy $\pi$.
The parameters of $\idfn$ are considered constant with respect to $\pi$.
The reward $r_{\idfn}$ of the action $a_t$ includes the system identification accuracy
of all $K$-windows containing $t$, as follows:
\begin{equation}\begin{split}
r_{\idfn}(s_t, a_t) = \frac{1}{K}
\sum_{t' = t - K + 1}^t
\| \idfn(\tau_{t':t'+K}) - \embedfn(s_d) \|_2^2
\label{sysid-reward}
\end{split}\end{equation}
where the error is defined as zero for out-of-bound windows.
%and a KL divergence term encouraging the distribution of the embeddings
%to match the first two moments of the unit normal distribution.
%The latter ensures that the supervised learning data sets presented to the identification function $id$ do not require any separate whitening step.
%\TODO{better motivation? It also helps avoid the collapse to a constant function,
%but technically we already ``solve'' that by adding the estimation loss to the RL objective...}
The system identification function $\idfn$ is not updated by the RL algorithm.
It is updated in a separate supervised learning step, performed after each iteration of RL.
We add entropy regularization $\cH(\pi)$ to $J(\pi)$ to stabilize the use of stochastic policy gradient RL algorithms.
In addition, we add fixed Gaussian noise to the output of $\embedfn$ using the reparameterization trick; this encourages robustness against small errors in system identification at test time.

In our experiments, $\idfn$ is parameterized by a one-dimensional convolutional neural network.
The convolutional architecture is chosen to allow longer $K$ without requiring a large number of parameters.
Additionally, it matches the intuition that differentiation and/or integration of the state and action trajectories
is often required to estimate the underlying dynamics parameters.
The remaining functions are parameterized by fully-connected neural networks.
We train $\pi$ with on-policy stochastic policy gradient RL algorithms
that utilize an advantage function estimator $A^\pi (s, a)$ to decrease the variance of the policy gradient estimate.
We implement $A^\pi$ as a function of the task state $s_t$ and the embedding value $\embedfn(s_d)$,
allowing $A^\pi$ to benefit from the disentangled latent features.
Our training process is summarized in Algorithm~\ref{algo}.
Further details of the neural networks are given in~\secref{implementation}.


\begin{algorithm}[hb]
\caption{Embed to Identify (E2ID)}
\begin{algorithmic}
\For{\text{\emph{fixed number of iterations}}}
  \State sample dynamics parameters $s_D^1,\ \dots,\ s_D^N$ uniformly from $\cS_D$
  \State collect trajectories $\tau^1,\ \dots,\ \tau^N$ and task rewards from $\pi$ for each of the $N$ parameters
  \State evaluate $\idfn$ to estimate embedding $\hat e_D$ for sliding $k-$window over each $\tau$
  \State compute $r_{\idfn}$ according to \eqref{sysid-reward} and add to task rewards $r(s_t,a_t)$
  \State update $\pi$ using on-policy RL for total reward $J(\pi)$ \eqref{objective-full}
  % $r = r_T + \alpha_{\cH}\cH(\pi) + \alpha_{ID} \E_{\pi} (\hat e_D - e_D(s_D))^2$
  \State update system identification network $\idfn$ using supervised learning
\EndFor
\end{algorithmic}
\label{algo}
\end{algorithm}


\begin{comment}
\begin{table}[ht]
\centering
\begin{tabular}{lll}
Type     & Filters & Size \\
\hline
Input    & --           & dim($\cS \times \cA$) \\
Conv1D   & 32           & 3 \\
Conv1D   & 32           & 3 \\
ReLu     & --           & -- \\
Max Pool & --           & 2 \\
Conv1D   & 32           & 5 \\
ReLu     & --           & -- \\
Linear   & dim($\latent$)   & -- \\
\end{tabular}
\caption{Architecture of 1D convolutional neural network used to learn the embedding estimator $id$.}
\label{conv1d}
\end{table}
\end{comment}


\section{Experiments}

\begin{figure}[ht]
\centering
\textbf{Training Time:}

\vspace{0.4cm}
\input{experiment_setup.tex}
\vspace{0.4cm}
\caption{
Overview of E2ID experimental setup.
At training time, correct dynamics parameters are available from the simulator.
A mapping $\embedfn$ from parameters to an abstract embedding space is learned,
along with a module $\idfn$ to estimate the embedding value from a state-action trajectory $\tau$.
The policy is rewarded for behavior that improves system identification accuracy.
At testing time, the true dynamics parameters are no longer known,
and the estimated embeddings are input directly to the policy.
}
\label{fig:overview}
\end{figure}

In this section, we illustrate the desirable properties of the learned embedding space $\latent$
and compare the performance of our architecture (denoted \embed) against several baselines:
\begin{enumerate}
\item \blind: A ``blind'' policy that has no access to $s_D$ in any form, representing the domain randomization.
\item \plain: A policy conditioned on the true system identification parameters $s_D$ instead of the learned embedding, similar to~\citet{yu-up-osi-rss17}, but with an extra fully connected layer between $s_D$ and $\pi$, such that the total number of policy parameters is equal to \embed.
$\idfn$ is still trained to estimate $s_D$.
%\item \traj: A policy where the state-action trajectory is input through a 1D convolutional network (identical to the SysID network) in training, without any condition that this network should estimate $\cS_D$ or an embedding.
\end{enumerate}
The structure of our experimental setup is illustrated in~\figref{fig:overview}.
At training time, ground truth $s_D$ values are available.
The embedding map $\embedfn : s_D \mapsto \latent$ 
and the system identification function $\idfn$ are learned.
At testing time, the true $s_D$ are no longer known,
and the estimated $s_D$ from $\idfn$ is input directly to the policy.
%The presented experimental results suggest that our embedding framework learns policies that generalize better than the alternatives \TODO{fingers crossed.}

\subsection{Point-Mass Environment}
\label{pointmass}
The point-mass environment serves as a ``simplest possible'' scenario to demonstrate our approach.
The low dimensionality of this system allow us to visualize the properties of the learned embedding more easily.
The state space $\cS = (p \in \R^2, v \in \R^2)$ is position and velocity of the unit mass in 2D.
The action space $\cA = [-1, 1]^2$ is a bounded force vector.
The system identification space $\cS_D$ is a gain factor $g \in \pm[0.175, 1.75]$.
such that the true force applied to the mass is $g \cdot a$.
The system dynamics are:
\begin{equation}\begin{split}
\dot p = v, \quad \dot v = -\epsilon v + ga
\end{split}\end{equation}
where $0 < \epsilon \ll 1$ is a fixed friction parameter.
The policy is trained to push the mass towards the origin
by the reward function $r = -\|p\|_1$.
%by the reward function $r = -\sqrt{|g|} \cdot \|p\|_1$,
%where the scaling factor $\sqrt{|g|}$ balances the rewards in the low- and high-gain scenarios
%based on the minimum-time bang-bang solution to the implied optimal control problem.
Due to the unknown sign of the gain factor, the \blind{} policy cannot possibly perform well in this environment.
However, since the gain parameter $g$ is trivial to estimate,
there is no reward difference between \embed{} and \extra{}.
%\TODO{better explanation, avoid readers thinking it's too contrived}

%The point-mass learning curves are shown in \TODO{figure}.
%As expected, the \emph{blind} policy performs very poorly.
%The \emph{plain} and \emph{extra} policies perform in between.

\begin{comment}
\begin{table}
\centering
\begin{tabular}{l c c c c}
       &          & test         & reward $\sigma$          & reward $\sigma$       \\
flavor & $\alpha$ & reward $\mu$ &            (per-episode) &            (per-seed) \\
\hline
\blind & N/A & -80.3 & 27.6 & 2.6 \\
\plain & 0.1 & -70.8 & 28.2 & 2.4 \\
\extra & 0.1 & -67.1 & 34.6 & 2.7 \\
\embed & 0.1 & \textbf{-62.7} & 27.6 & 0.8 \\
\end{tabular}
\end{table}
\end{comment}


\begin{figure}
\centering
\includegraphics[trim={13.7cm 0 2cm 0}, clip, width=0.35\textwidth]{pointmass_embed_mapping.pdf}
\includegraphics[trim={0.5cm 0 1cm 0}, clip, width=0.63\textwidth]{pointmass_embed_scatter.pdf}
\caption{
\emph{Left}: One dimension of learned mapping $\embedfn$ from gain factor $g$ to 2D embedding space $\latent$ in point-mass environment.
\emph{Middle}: Actual vs. estimated gain parameter $g$ for \plain{} policy.
\emph{Right:} Actual vs. estimated learned embedding in $\latent$ with \embed{} policy (one of two dimensions).
Embedding spreads positive and negative gain to more distant clusters.
}
\label{embed-mapping}
\end{figure}


\figref{embed-mapping} illustrates the learned mapping from the gain factor $g$
to the embedding space $\latent$.
The scatter plots compare the ground truth embedding values
against those estimated by $\idfn$ in the \embed{} policy,
and the ground truth gain factor $g$ against the values estimated by the equivalent of $\idfn$ in the \plain{} policy.
%Against the $x-$axis, density estimates are shown.
Both illustrate that the learned embedding function $\embedfn$ ``squashes'' the gain factor $g$ into positive and negative clusters.
The separation between these clusters is magnified, making it easier for the policy to switch between two disjoint behavior styles depending on the sign of the gain factor.
The ``squashing'' does not impede the task reward of the policy,
since an identical bang-bang control policy is optimal for any magnitude of $g$ -- only the sign of the input needs to change.

Figure~\ref{fig:conditional_action} shows the policy output $\pi(a|s_D)$ conditioned on
the gain factor $g$ at the initial task state $s_{t0} = (1, 1)$.
This illustrates that the policy contains two disjoint behaviors,
and switches between these behaviors rapidly and precisely with the sign of the gain $g$.



\begin{figure}
\centering
\includegraphics[width=0.85 \textwidth]{pointmass_conditional_action.pdf}
\caption{
Action of trained \embed{} policy at initial state (position $ = (1,1)$, velocity $=(0,0)$) conditioned on gain factor $g$.
Outputs are clipped to valid action range.
Plots show that policy has learned the optimal bang-bang control policy in both positive and negative gain scenarios.
}
\label{fig:conditional_action}
\end{figure}

\subsubsection{Disentangling of redundant features}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{embed_colors.pdf}
\caption{
Illustration of learned embedding space that disentangles redundant dynamics parameters.
Graph axes represent true dynamics parameters; colors represent learned embedding values.
In point-mass environment with parameters of gain $g$ and mass $m$,
the dynamics only depend on the $g/m$ ratio.
The learned embedding reflects this; for example, the $g=m$ line is all mapped to similar embedding values.
Plot restricted to positive gain domain for clarity.
}
\label{fig:embed_colors}
\end{figure}
A primary benefit of our approach is its ability to distill potentially complex sets of dynamics parameters
into a simplified space where only the information needed to achieve good rewards is preserved.
We illustrate this by constructing a version of the point-mass environment with redundant features:
we add a mass parameter $m$ alongside the gain parameter $g$.
Now, the true acceleration of the point is proportional to $g/m$,
making all $(g, m)$ combinations with the same ratio indistinguishable to system identification.
This poses a problem for the \plain{} or \extra{} frameworks
because the learning problem of recovering the true $(g, m)$ from a state-action trajectory is ill-posed.
In contrast, our framework learns an embedding where all $(g, m)$ combinations with similar ratios are mapped to similar embedding values.
This embedding is visualized in Figure~\ref{fig:embed_colors}.

\begin{comment}
\begin{figure}
\centering
\TODO{learning curves for redundant point-mass.}
\caption{
Example of incompatibility between our observability objective \TODO{ref}
and attempting to reconstruct the true dynamics parameters
in redundant point-mass environment.
When $\alpha = 0.1$, \extra{} network alters its behavior in pursuit of the observability reward, but the dynamics parameters are impossible to estimate.
The learning process is disturbed.
When $\alpha = 0$, \extra{} network does not alter its behavior
and learns a policy that accounts for the many-to-many characteristic of the system identification problem.
}
\label{fig:redundant_fail}
\end{figure}
\end{comment}


Note that the learned embedding in these examples is two-dimensional -- 
the human operator does not need to know the minimal dimensionality of the parameters after redundancies are removed.
This is important in scenarios with many parameters where identifying redundant parameters 
is not as simple as in the point-mass environment.

\subsection{Half-Cheetah Environment}

\begin{figure}[ht]
\includegraphics[trim=4cm 3cm 0cm 4cm, clip, width=0.22\textwidth]{cheetah_short.png}\hfill
\includegraphics[trim=4cm 3cm 0cm 4cm, clip, width=0.22\textwidth]{cheetah_medium.png}\hfill
\includegraphics[trim=4cm 3cm 0cm 4cm, clip, width=0.22\textwidth]{cheetah_backleg.png}\hfill
\includegraphics[trim=4cm 3cm 0cm 4cm, clip, width=0.22\textwidth]{cheetah_long.png}
\caption{Variations of Half-Cheetah environment produced by randomization of kinematic and dynamic properties.}
\label{cheetahs}
\end{figure}

As an example of a more complex task, we demonstrate results on the \emph{Half-Cheetah}
planar locomotion environment from the OpenAI Gym~\citep{openai-gym}.
The following parameters are randomized:
lengths of seven kinematic links,
damping, stiffness, and ranges of six planar revolute joints,
and gear ratios of six rotary actuators.
In total, 37 parameters are randomized.
The embedding space $\latent$ is chosen to be 8-dimensional.
Each end of the joint range is shifted by $\pm 0.3$ radians.
All other parameters are multiplied by a ratio log-uniformly distributed in the range $[\beta^{-1}, \beta]$ with $\beta = 1.$.

Due to the architecture of the MuJoCo physics simulator used in this environment~\citep{todorov-mujoco},
it is not practical to sample new random dynamics parameters $s_D$ for each training iteration.
Instead, we train on a fixed set of 64 randomized models.
At test time, we sample a new, independent set of 64 randomized models.
%Instead, we construct a ``universe'' of 256 models initially, and randomly choose 8 of these environments for each training iteration.
%At test time, we resample a new ``universe'' of models.

Results from these experiments are shown in Table~\ref{cheetah}.
The data show that, while the \extra{} policy performs slightly better in training,
it suffers from worse generalization error.
The \embed{} policies achieves lower generalization error and higher test rewards.
Note that the training, as well as the test, performance for nonzero $\alpha$
is better for both \embed{} and \extra{} policies.
We hypothesize that behaving the same for all $s_D$ values, like the \blind{} policy,
is a local optimum in the policy space,
whereas the optimal policy specializes for each $s_D$,
but this optimum is more difficult to find.
The observability reward provides an additional incentive
for the policy to behave differently for environments with different $s_D$ values,
pulling it away from the undesirable local optimum.

\begin{table}[ht]
\centering
\begin{tabular}{l l l l l l l}
Type & $\alpha_{id}$ & $\mu_{train}$ & $\sigma_{train}$& $\mu_{test}$ & $\sigma_{test}$ & generalization error \\
\hline
\embed{} & 0 & 336.0 & 167.7 & 266.9 & 145.8 & -69.1 \\
\embed{} & 0.01 & 363.2 & 170.5 & 288.6 & 145.5 & -74.6 \\
\extra{} & 0.0 & 354.9 & 152.2  & 263.7 & 147.4 & -91.2 \\
\extra{} & 0.01 & 370.9 & 149.6 & 274.5 & 141.4 & -96.4 \\
\blind{} & N/A & 266.1 & 156.7 & 244.1 & 140.6 & -22.0
\end{tabular}

\vspace{0.4cm} \\

\caption{
Training and test rewards in half-cheetah environment for \blind{}, \extra{}, and \embed{} policies.
Learned embedding space gives significant improvement.
Observability reward $\alpha$ gives moderate improvement.
}
\label{cheetah}
\end{table}

These results suggest that learning the latent space $\latent$ is beneficial
both for the training process and for the test performance.


\subsection{Architecture and Implementation Notes}
\label{implementation}
In all experiments, we use Proximal Policy Optimization (PPO)~\citep{schulman-ppo} as our reinforcement learning algorithm.
The policy $\pi$ is parameterized as a fully connected neural network with 3 hidden layers of 128 units each, using ReLU nonlinearities.
The advantage function used as a baseline by PPO uses an identical network.
The embedding function $\embedfn$ network contains 2 hidden layers with 74 units each.
The system identification function $\idfn$ is composed of three 1D-convolutional layers, each with 64 filters of width 3 and ReLU activation, followed by a single fully connected layer with 128 units, and a linear output layer.


\section{Conclusion}
In this paper, we demonstrated a novel framework for training robot control policies
that can adapt online in real time to variations in system dynamics parameters.
We learn an embedding space that distills the full system identification information
and can disentangle redundant and unobservable parameters, while providing only the information that is useful for decision making.
We introduce an observability reward function in the reinforcement learning reward,
such that the control policy is encouraged to behave in a way that makes system identification easier.
Experimental results illustrate the desirable properties of the embedding space on a low-dimensional problem
and demonstrate improved training performance and test generalization compared to baselines
where the embedding space and observability reward are not used.
Future work will focus on applying this method to simulation-to-reality transfer on a real robot.
We also expect to see a greater benefit from the embedding space on more complex systems with very large system identification spaces.

\clearpage
\acknowledgments{}
\bibliography{bibliography}{}

\end{document}
