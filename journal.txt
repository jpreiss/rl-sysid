02-05
-----
To establish baseline, tested TRPO on Reacher environment.
With fixed goal, mastered (rew. around -6) in ~150 iterations.
With moving goal, close after ~200 iterations but not quite mastered.

Next: TRPO, fixed goal, randomized kinematics. Problem is much harder.
After 200 iterations still is far from converged, often see spinning, etc.
Seems to plateau at avg reward of around -10 after ~150 iterations.
Entropy is very low.

Task length makes big difference: accidentally tried fixed goal / random kinematics
with 128 timesteps - saw total failure, crazy spinning, etc.

Possible: batch mode needed to get experience with different kinematics
in same TRPO update.
Or, keep it the same but slowing down learning rate.

Terminated after 440 iterations, still stuck at about same performance as 150 iters.

Created new wrapper that changes kinematics each episode -
will test hypothesis of line 15.
--> This worked. Mastered in about 150 iterations with randomization ratio 1.5.
logged in trpo_kinematic_fixedgoal_1.
Now trying with ratio of 2.
Clearly harder but still improving at 120 iters.
... getting closer at 240, but still avg reward of around -8.5, worse than before...
... gets around -7.5 by 320 iters. Looks like still improving.
also possible that longer links are reducing best possible avg reward a little.
Terminated because doing quite well and want to try moving target w/ 1.5 kimenatic ratio.
logged in trpo_kinematic_fixedgoal_2.

Trying ratio 1.5 with moving goal.
Iteration 120, avg reward around -11, roughly same as ratio 2 with fixed goal.
Iteration 300, avg reward around -9, still strugging.
Entropy down to -2.4, wondering if stronger entropy regularization is needed.
500 iterations, reward a little above -8 but learning v slowly. Terminating.
logged in trpo_kinematic_fixedgoal_3. 

Trying increase in entropy regularization: 0.01 to 0.05.
Still 1.5 ratio w/ moving goal.
logged in trpo_kinematic_fixedgoal_4.

Also halving max-KL: 0.01 to 0.005.
This is with original ent reg of 0.01.
logged in trpo_kinematic_fixedgoal_5.

Neither working too well -
increased entropy seems plateaued around -15,
Decreased KL seems plateaued around -10

High entropy is clearly not working, will terminate. lower KL seems like it might be working.

Realized made a mistake, ratio is 2 instead of 1.5. trying with 1.5
(entropy and KL both back to default settings)
logged in trpo_kinematic_fixedgoal_6.

lower KL is definitely learning a reasonabl policy, but not great
(sometimes gets close but not actually touching target)
unclear if it would eventually learn perfect policy...

Wondering if short episode length occasionally makes it not worth it
to expend the control needed to get long arm to target
(because long arm == more mass == higher control signals needed.)
Maybe actuator penalty should be normalized by arm mass...

Figure idea:
w fixed goal, show dramatically different configurations needed to touch -
emphasizes that policy is almost like ensemble w/switcher

after 770 iterations, ratio2 - slowrate is around avg reward of -7
but loooking at it execute, it still totally misses some targets

after 350 iterations, ratio1.5 - default is around -7.5
but quality is also kind of low
wondering if policy is really not expressive enough?

To try:
compare RELU,
deeper network
wider network


02-06
-----
trying with ctrl penaltly scaled according to sum(link length)**2.
Seems to help speed up learning a bit but still has trouble with "hard" configurations
like tight angles in elbow.
logged in trpo_kinematic_fixedgoal_7.

trying wider NN.
logged in trpo_kinematic_fixedgoal_8.
initial test seemed to get stuck going to center,
so reduced # of CG iters in TRPO. overwriting log.


relu 64, higher ent reg
180 iters, -13-12 reward
570 iters, -10 reward, but still slowly getting better
around -8 at 885 iters. seems good setting but too slow. Increase KL?
logged in trpo_kinematic_fixedgoal_10.
Increasing KL in _11.

02-08
-----
Talking to Artem. Randomized goal is not really necessary to show our method.
Will try just using fixed goal w/ random initial position.
Ultimately, we just need something where SysID info is necessary - this qualifies.
logged in trpo_kinematic_fixedgoal_12.
learns really well by 200 iterations with 1.5x joint length range, rew about -5.
with 2.0x:
def. harder. Learns pretty good policy by 300 iters, but still inefficient -
sometimes takes "long way around", burning more ctrl than needed.
policy's overal "strategy" is unimodal where it should be multimodal.
will try with higher entropy regularization.
learns more bimodal policy by 200 iters, reward not better yet though.
Policy really good by 500 iters. solved.
logged in trpo_kinematic_fixedgoal_13.

episode length 50->70 in _14.
strangely makes it much harder, still not even decent policy by 200 iters.
By 500 iters, rew around -8, policy pretty decent, but seems not helpful
in getting a policy that's better in the end.
Will experiment with 50 length for now.
logged in trpo_kinematic_fixedgoal_14.

now trying goal location fixed along y axis, random y coord only.
with random initial conditions, this is in a sense equivalent to fully randomized goal
if you re-orient the coordinate system.
Takes much longr to learn but looking pretty good around 400 iters.
Not perfect, totally fails on a few cases but generally gets close.
logged in trpo_kinematic_fixedgoal_15.

observed that goals very close to shoulder cause problems.
this is kind of degenerate case because of elbow angle limit.
making minimum distance a little further to avoid this.
logged in trpo_kinematic_fixedgoal_16.

also noticed somehow set vf_iters from 1 to 3 in trpo...
with this change, learns faster, but also entropy decreases faster...
unsure if will convrege as well...
... it does.

Implemented true batch TRPO. policy converges in about 100 iters!


02-09
-----
Tuned sysid_alpha constant, sysid optimization iters, to get decent sysid.
saw error reduction by factor of about 5 by adding sysid to RL objective,
but probably need a more thorough comparison to show that it actually helps.
further reduction of about 2-3x by switching from FC to conv1d architecture.
started with 2 layers of 16 5xK filters with len-2 max pooling.
upgrading to 32 filters helped, but only slightly.
switching to 3xK filters adding extra layer helps by factor of around 2x
now using (conv 3xK, conv3xK, relu, maxpool, conv 3xK, conv 3xK, relu, FC)
doesn't help much, just a little.
probably there's an error floor from once the hand reaches the goal.

Switched to embedding SysID instead of trying to reproduce true values.
Saw a variance collapse, all values are very close to the mean.
This makes sense, it allows the SysID network accuracy to be high
without actually learning anything. NNs always find shortcuts...


02-15
-----
Added a penalty term to the SysID loss: KL(empirical|Normal(0,1)).
This effectively forces the embedding space to not collapse.
Still running experiments to see how well the policy actually learns.
Seems to be converging to sub optimal too quickly.
The KL divergence penalty is quite small.
I think maybe the SysID loss itself is too big - the new normal values
are much bigger than the link lengths themselves,
so the SysID squared error is going to be bigger too.

This is worth thinking about.
The KL penalty means you only need to tune alpha_KL once.
With raw un-normalized SysID values, you need to tune for every scenario.
But I think this can be fixed in the raw SysID case by 
using a running mean/var estimate and making the network
reproduce the normalized values instead.

Cutting alpha_KL to 1e-2 helps learning, but at 67 iters sysid still bad...
...policy good by 100 iters but sysid still not great...
...at 200 iters, sysid around 0.01 - but sqrt(0.01) == 0.1, could be better...
...trying higher penalty...

02-17
-----
Finally was able to get all the different baselines working.
The SysID network is trained to reconstruct the normalized SysID values.
Restructured the code, wrote outer loop code to run experiments.
However, it seems that all policies - even the "blind" policy with no access
to the link lengths at all - perform well.

02-19
-----
Tried a few things to differentiate polcies.
Entropy rgularization was high - random movement probably caused good SysiD
performance at test time without trying.
Experimented with decreased control penalty, but it makes the policies too crazy.
Cut control penalty by half because found it makes learning faster.

Thinking that if we observe *distance* to the goal instead of vector,
that might make SysID important enough. But how to justify...

02-22
-----
Even if we only observe the distance, ``blind'' still does well!
At least now, we see clearly better performance for the SysID-capable networks
when alpha_sysid > 0, but not any better than blind.
Somehow, this problem is too easy.


04-25
-----
WTF happened to my life?
Anyway...
Constructed pointmass batch environment.
Verified it's learnable with constant sysid params but initializing the mass anywhere in a box - 
so the difficulties with radial symmetry we saw before are not too bad here.
Now randomizing the gain of the control input - *including negative values* !
I thought this would be a good test case because it should be impossible to make any progress without some kind of SysID.
However, I was shocked to find that the "blind" policy can still learn to make progress here!
It is definitely not great, it does a kind of corkscrew towards the goal, and sometimes flies off in the other direction.
But how is it even getting close to the goal?
Is it somehow encoding information in the velocity, or the angle above/below the line to the goal?

OK, but the performance here is still not great.
A policy with access to the gain SysID parameter should get much better rewards.

I first tried with the MLP that simply has access to the true SysID value.
Surprisingly, this did not do that well.
I wonder if it is possible that the 2 layer MLP is not capable of assigning such importance to a single weight?

I then switched to the embedding network ("ours").
It performs very well, much better than either of the previous networks.
Some kind of code error happened when I made the embedding space 1D, so I switched to 2D.
That is probably a bug, TODO: figure it out.

I need to figure out why the "PLAIN" network that can see the true value isn't doing better.
This seems like a bug.

Trying with the "EXTRA" network that gets some additional layers to preprocess the sysid info, but tries to recover the true SysID info instead of recovering the embedding.
This one also does equally bad as the "PLAIN" network.
Hmmm... maybe something is happening with the SysID loss?
If that's the issue, I should be able to set the SysID loss to zero and see the PLAIN network's performance improve.

...yes, it improves to be as good as the EMBED network.
So there is something messed up about the SysID loss for non-EMBED networks.
Or (better for us!) the embedding is somehow chosen to make it easier to recover!!

In PLAIN, SysID loss was in the 0.005 - 0.012 range.
In EMBED, SysID loss was in the 0.05 - 0.1 range.
This is weird... I was thinking that PLAIN might have a larger SysID loss
and that was making it do weird behaviors to make the SysID values more observable.
But it's the opposite.
More explanation is needed...

Now trying to train PLAIN with 10x larger SysID loss.
This makes it much worse with crazy behavior.
There must be a bug in computation of the non-EMBED SysID loss.


04-27
-----
Was sick yesterday.

Made plots of action conditioned on gain (passing through embedding)
and saw exactly what we want - higher force for low gain, with a sign change thru zero.
Strangely, the x and y actions are different.
Would be interesting to explore same plot at different mass positions.

Tried to find bug in the non-embedding SysID loss, but haven't found one yet.
Noticed that policies learn to drive the mass to the origin, but then keep wiggling around.
Is this a learned policy to continue making the SysID variables observable?
If so, why don't we also see it for the embedding network?
The embedding network generally does nothing once the mass is at the origin.

Also, I'm still confused as to why the SysID loss is so much lower for the true values
than for the embedding.
The SysID network tries to reconstruct the *whitened* SysID variables, and we take the loss after whitening as well.
So it doesn't matter whether we're using the embedding or not, the SysID error should always be comparing against a unit Gaussian.

Is it possible that the policy has learned to trade off some accuracy of its embedding to gain some performance on the main task reward?
If so, why cant the non-embedding networks learn to do the same thing?


< Early May >
-------------
Have achieved main desired results in point-mass environment.
Most interestingly, we added a redundant (unobservable) parameter,
so the params are mass and gain, but in the end only gain/mass ratio matters.
We then show that the learned embedding "disentangles" the unobservable parameters
such that all combinations with equal mass/gain ratios are mapped to the same embedding value.
Unfortunatley, we can't then show that this results in improved performance at test time,
because it seems that the policy just learns to do the right thing despite
the fact that the SysID network could output many different gain/mass estimates for
the same observed state/action trajectory.

(TODO: would be cool to generate a bunch of different trajectories for a fixed gain/mass ratio
and see how the SysID network's output varies.
Does it just output e.g. mass of 1 no matter what?)


< Early-Mid May >
-----------------
Having trouble with Half-Cheetah.
Randomizing link lengths, joint ranges, stiffness, damping, gear ratios.
Was seeing just not very good rewards even with randomization turned down.
Noticed that value function is converging very fast but then error starts
slowly, but steadily increasing.
This happens after moving from negative rewards (mostly flailing)
to positive rewards (significant forward progress).
After VF error starts growing, policy rewards stall or get worse.
Tried turning down VF learning rate... still not learning that good performance though.


May 16
------
After a few experiments with baseline TRPO and non-batch enviro,
found that too-high entcoeff seems to damage learning significantly.
Entcoeff turned down to 0.01 gives good results with no randomization.
With ep len of 196, N=64, PPO, ent=0.01, learns avg rew close to 500 after 200 iters.
This is with blind policy.
Interestingly, the value function error is quite large (~400).
Adding in moderate amounts of randomization, will see if it still reaches positive rewards...
... yes. The learning is a little slower and the variance depending on the SysID sample
makes the variance of rewards between iters much higher, but it seems a lot better than higher entcoeffs.
Now doing one more experiment with no randomization,
testing if higher vf learning rate changes anything...
... no, is just as bad. Looking back at previous experiment,
it was doing the same converge quick - diverge a lot behavior.
But that was already with a 1:1 ratio btw vf and policy learning rates.
Does this mean we need the vf rate to be *slower*?
--> also, I still wonder if ADAM is the best choice for the VF.
The momentum mechanism seems like it could get really screwed up by the non-stationary input distribution.
... 0.25x learning rate still shows same behavior...
... 0.05x learning rate, the same thing happens, but in slow motion.

It seems like something more fundamental wrong is happening.
I'm starting to think that plain SGD would be better for the VF.
Trying plain SGD - the same phenomenon happens, error grows by a lot once we learn how to get positive rewards.
Maybe it's just a characteristic of the environment?
I guess the absolute value of the positive rewards once we learn how to run
are a lot larger than those of the negative rewards when we're flailing.
And the vf loss is squared error, so it blows up a lot.

Anyway, since we can learn a good policy in <= 200 iters with either Adam or SGD,
despite the big vf_loss, I'm going to stop worrying about it for now.


Sept 11
-------
TODO:
- sweep embed dim with multiple seeds
- look into q fn error / variance
- better environment
- PPO-blind vs. SAC-ours
- why is embed worse than plain
- writing about our "insight"

